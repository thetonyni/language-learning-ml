{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Tony Ni\n",
    "\n",
    "Project Prototype/Draft\n",
    "\n",
    "Title: Language Learning Abilities: Classification and Factor Analysis\n",
    "\n",
    "I was inspired by reading an article which discussed the language learning abilities of individuals from all over the world. The investigators of that study collected information including demographic variables of the participants, information regarding their language abilities, and the score they received from an admnistered English grammar test. I'm hoping to use the information gathered from those researchers to aid me in creating a classification model to predict an individual's `skill_group` what score one would expect someone to receive if they took the English grammar test. \n",
    "\n",
    "I will be running some weak learners such as a perceptron, logistic regression classifier, random forest etc. and seeing which creates the model with the highest accuracy (as my metric for measuring the effectiveness of a model). I'm expecting the perceptron and logistic regression classifer to do poorly and the random forest model to overshadow them in terms of effectiveness. \n",
    "\n",
    "Due to the large amounts of variables within the dataset, I am also interested in investigating complex dynamics and relationships between these variables. For example, if there is some sort of underlying/common thread connecting certain variables to one another (ex: socioeconomic status could be an underlying \"factor\" which consists of the variables \"class,\" \"income,\" \"highest level of education completed,\" etc. through factor analysis. I am hoping to find a way to choose a number of factors to create, and then create loadings for each factor for all the variables in order to see which variable is the most important/distinct to each factor, which I will try to make interpretations of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>natlangs</th>\n",
       "      <th>primelangs</th>\n",
       "      <th>dyslexia</th>\n",
       "      <th>psychiatric</th>\n",
       "      <th>education</th>\n",
       "      <th>Eng_start</th>\n",
       "      <th>Eng_country_yrs</th>\n",
       "      <th>house_Eng</th>\n",
       "      <th>dictionary</th>\n",
       "      <th>already_participated</th>\n",
       "      <th>currcountry</th>\n",
       "      <th>nat_Eng</th>\n",
       "      <th>speaker_cat</th>\n",
       "      <th>type</th>\n",
       "      <th>Lived_Eng_per</th>\n",
       "      <th>Eng_little</th>\n",
       "      <th>correct</th>\n",
       "      <th>skill_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>18-29</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Undergraduate Degree (3-5 years higher ed)</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>&gt; 0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>30-44</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Portuguese, Spanish</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate Degree</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0.8-0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>18-29</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>Dutch, English</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate Degree</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Netherlands</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>little</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>&gt; 0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>30-44</td>\n",
       "      <td>Finnish</td>\n",
       "      <td>Finnish</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Some Undergrad (higher ed)</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Finland</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Finnish</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>&gt; 0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>18-29</td>\n",
       "      <td>Estonian</td>\n",
       "      <td>English, Estonian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Some Undergrad (higher ed)</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ireland (Republic of)</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Estonian</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>little</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.8-0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211911</th>\n",
       "      <td>23</td>\n",
       "      <td>18-29</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>Dutch, English</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Some Graduate School</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.8-0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211912</th>\n",
       "      <td>39</td>\n",
       "      <td>30-44</td>\n",
       "      <td>French</td>\n",
       "      <td>French</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>High School Degree (12-13 years)</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Italian</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>&lt; 0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211913</th>\n",
       "      <td>22</td>\n",
       "      <td>18-29</td>\n",
       "      <td>Russian</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate Degree</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Russia</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0.8-0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211914</th>\n",
       "      <td>22</td>\n",
       "      <td>18-29</td>\n",
       "      <td>Polish</td>\n",
       "      <td>English, Polish</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate Degree</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Polish</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>&gt; 0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211915</th>\n",
       "      <td>29</td>\n",
       "      <td>18-29</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate Degree</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>0</td>\n",
       "      <td>late</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>little</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.8-0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211916 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age age_group    natlangs           primelangs  dyslexia  psychiatric  \\\n",
       "0        20     18-29  Vietnamese           Vietnamese         0            0   \n",
       "1        37     30-44     Spanish  Portuguese, Spanish         0            0   \n",
       "2        29     18-29       Dutch       Dutch, English         0            0   \n",
       "3        39     30-44     Finnish              Finnish         0            0   \n",
       "4        24     18-29    Estonian    English, Estonian         0            0   \n",
       "...     ...       ...         ...                  ...       ...          ...   \n",
       "211911   23     18-29       Dutch       Dutch, English         0            0   \n",
       "211912   39     30-44      French               French         0            0   \n",
       "211913   22     18-29     Russian              Russian         0            0   \n",
       "211914   22     18-29      Polish      English, Polish         0            0   \n",
       "211915   29     18-29     Spanish              Spanish         0            0   \n",
       "\n",
       "                                         education  Eng_start  \\\n",
       "0       Undergraduate Degree (3-5 years higher ed)          6   \n",
       "1                                  Graduate Degree         18   \n",
       "2                                  Graduate Degree         12   \n",
       "3                       Some Undergrad (higher ed)         11   \n",
       "4                       Some Undergrad (higher ed)          7   \n",
       "...                                            ...        ...   \n",
       "211911                        Some Graduate School         10   \n",
       "211912            High School Degree (12-13 years)         14   \n",
       "211913                             Graduate Degree          7   \n",
       "211914                             Graduate Degree          6   \n",
       "211915                             Graduate Degree         10   \n",
       "\n",
       "        Eng_country_yrs  house_Eng  dictionary  already_participated  \\\n",
       "0                     0          1           0                     0   \n",
       "1                     0          0           0                     0   \n",
       "2                     1          0           0                     0   \n",
       "3                     0          0           0                     0   \n",
       "4                     1          0           0                     0   \n",
       "...                 ...        ...         ...                   ...   \n",
       "211911                0          0           0                     0   \n",
       "211912                0          0           0                     0   \n",
       "211913                0          0           0                     0   \n",
       "211914                0          0           0                     0   \n",
       "211915                0          0           0                     0   \n",
       "\n",
       "                  currcountry  nat_Eng speaker_cat        type  Lived_Eng_per  \\\n",
       "0                     Vietnam        0        late  Vietnamese       0.000000   \n",
       "1                      Mexico        0        late     Spanish       0.000000   \n",
       "2             The Netherlands        0        late       Dutch       0.058824   \n",
       "3                     Finland        0        late     Finnish       0.000000   \n",
       "4       Ireland (Republic of)        0        late    Estonian       0.058824   \n",
       "...                       ...      ...         ...         ...            ...   \n",
       "211911            Netherlands        0        late       Dutch       0.000000   \n",
       "211912                 France        0        late     Italian       0.000000   \n",
       "211913                 Russia        0        late     Russian       0.000000   \n",
       "211914         United Kingdom        0        late      Polish       0.000000   \n",
       "211915              Argentina        0        late     Spanish       0.000000   \n",
       "\n",
       "       Eng_little   correct skill_group  \n",
       "0          little  0.957895      > 0.95  \n",
       "1          little  0.831579    0.8-0.95  \n",
       "2          little  0.978947      > 0.95  \n",
       "3          little  0.989474      > 0.95  \n",
       "4          little  0.873684    0.8-0.95  \n",
       "...           ...       ...         ...  \n",
       "211911     little  0.852632    0.8-0.95  \n",
       "211912     little  0.789474      < 0.80  \n",
       "211913     little  0.831579    0.8-0.95  \n",
       "211914     little  0.968421      > 0.95  \n",
       "211915     little  0.915789    0.8-0.95  \n",
       "\n",
       "[211916 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "#reading in data\n",
    "df = pd.read_csv('languagelearning_cleaned.csv')\n",
    "df = df.drop(df.columns[[0, 1, 2]], axis=1)  # drop id columns\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>natlangs</th>\n",
       "      <th>primelangs</th>\n",
       "      <th>psychiatric</th>\n",
       "      <th>education</th>\n",
       "      <th>Eng_start</th>\n",
       "      <th>Eng_country_yrs</th>\n",
       "      <th>house_Eng</th>\n",
       "      <th>currcountry</th>\n",
       "      <th>nat_Eng</th>\n",
       "      <th>Lived_Eng_per</th>\n",
       "      <th>Eng_little</th>\n",
       "      <th>correct</th>\n",
       "      <th>skill_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2319</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>2249</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1905</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.989474</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1016</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211911</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211912</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1954</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211913</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>2260</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.831579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211914</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1793</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>292</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211915</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>2289</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211916 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  age_group  natlangs  primelangs  psychiatric  education  \\\n",
       "0        20          0        42        2319            0         22   \n",
       "1        37          1        36        2249            0          8   \n",
       "2        29          0        10         754            0          8   \n",
       "3        39          1        14        1905            0         21   \n",
       "4        24          0        12        1016            0         21   \n",
       "...     ...        ...       ...         ...          ...        ...   \n",
       "211911   23          0        10         754            0         20   \n",
       "211912   39          1        16        1954            0         10   \n",
       "211913   22          0        32        2260            0          8   \n",
       "211914   22          0        29        1793            0          8   \n",
       "211915   29          0        36        2289            0          8   \n",
       "\n",
       "        Eng_start  Eng_country_yrs  house_Eng  currcountry  nat_Eng  \\\n",
       "0               6                0          1          300        0   \n",
       "1              18                0          0          207        0   \n",
       "2              12                1          0          277        0   \n",
       "3              11                0          0          112        0   \n",
       "4               7                1          0          167        0   \n",
       "...           ...              ...        ...          ...      ...   \n",
       "211911         10                0          0          219        0   \n",
       "211912         14                0          0          124        0   \n",
       "211913          7                0          0          244        0   \n",
       "211914          6                0          0          292        0   \n",
       "211915         10                0          0           17        0   \n",
       "\n",
       "        Lived_Eng_per  Eng_little   correct  skill_group  \n",
       "0            0.000000           0  0.957895            2  \n",
       "1            0.000000           0  0.831579            0  \n",
       "2            0.058824           0  0.978947            2  \n",
       "3            0.000000           0  0.989474            2  \n",
       "4            0.058824           0  0.873684            0  \n",
       "...               ...         ...       ...          ...  \n",
       "211911       0.000000           0  0.852632            0  \n",
       "211912       0.000000           0  0.789474            1  \n",
       "211913       0.000000           0  0.831579            0  \n",
       "211914       0.000000           0  0.968421            2  \n",
       "211915       0.000000           0  0.915789            0  \n",
       "\n",
       "[211916 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label encoding vategorical variables\n",
    "le = LabelEncoder() \n",
    "\n",
    "#fixing \n",
    "df[\"currcountry\"] = le.fit_transform(df[\"currcountry\"].astype(str))\n",
    "    \n",
    "#list of categorical variables\n",
    "categorical_variables = [\"age_group\", \"natlangs\", \"primelangs\", \"education\", \"currcountry\", \"speaker_cat\", \"type\", \"Eng_little\", \"skill_group\"]\n",
    "\n",
    "#label encoding\n",
    "for var in categorical_variables:\n",
    "      df[var] = le.fit_transform(df[var])    \n",
    "\n",
    "#drop na and inf\n",
    "df = df[np.isfinite(df).all(1)]\n",
    "\n",
    "#change to float\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n",
    "#remove variables (which were causing errors in linear algebra in FA due to lack of nonzero entries)\n",
    "df = df.drop(columns=[\"dyslexia\", \"dictionary\", \"already_participated\", \"speaker_cat\", \"type\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dropped some of the variables which did not have an entry in the data dictionary from which I obtained the dataset from, as without knowing what the variable is or what sort of information it contains, it generally unwise and unable to be used during our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "a = df.to_numpy() #coerces dataframe to numpy list\n",
    "X = a[:, 0:13] #gets all numerics\n",
    "y = a[:,14] #class\n",
    "\n",
    "#splitting data in 70/30 training/testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for my classification task, I split the model up into a 70/30 training/testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in y: [128523  15391  68002]\n",
      "Labels counts in y_train: [89966 10774 47601]\n",
      "Labels counts in y_test: [38557  4617 20401]\n"
     ]
    }
   ],
   "source": [
    "y = y.astype(int)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train) ## Note that we standard only on the basis of the training set\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also standardized the variables to a (0,1) distribution as some of the variables were on completely different scales than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 26681\n",
      "Accuracy: 0.580\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=1)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity, I tried out the perceptron to see whether or not if it would perform well with my data -- it misclassifed around 26681 observations in the testing set which means it only correctly classified 58% of the examples. This is only somewhat better than by chance (> 0.5), but I'm sure there must be a better classification model out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 24966\n",
      "Accuracy: 0.607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=100.0, random_state=1,\n",
    "solver='lbfgs', multi_class='ovr')\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred_logistic = lr.predict(X_test_std)\n",
    "print('Misclassified examples: %d' % (y_test != y_pred_logistic).sum())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, trying a logistic regression model yields only a slight improvement on the perceptron classifier, rising to a 60.8% accuracy. Instead of using just weak learners, I figured perhaps an ensemble method such as random forests might be the way to go here in order, so I implemented a random forest classifer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified examples: 25561\n",
      "Accuracy: 0.598\n"
     ]
    }
   ],
   "source": [
    "# evaluate random forest algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X_train_std, y_train)\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred = model.predict(X_test_std)\n",
    "\n",
    "print('Misclassified examples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifiers didn't do _quite_ as well as I would have expected/wanted. The random forest classifier performs better than the perceptron implementation but worse than the logistic regression classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next portion of my project deals with factor analysis and trying to find latend variables amongst all the variables in the dataset which can hopefully allow us to find out which variables might have connections to one another and use that to further delve into the hidden information of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conduct Bartlett’s Test of Sphericity in order to make sure that the correlation matrix of the variables in our dataset is different enough from the identity matrix (with 1's in diagonal entires, and 0 elsewhere), so that factor analysis is OK to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1763935.7749880136, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bartlett’s Test of Sphericity\n",
    "\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(df)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the test, we obtained a chi-squared value of 1604546 and a p=value of 0, which tells us that the correlation matrix has statistically significant correlations among at least some of variables. Factor analysis should be able to be run on our data. Knowing this, we will begin our implementation of factor analysis. \n",
    "\n",
    "To begin, we need to know what is a reasonable number of factors to use, so it is a good idea to look at eigenvalues (for the factors), which is a measure of how much of the variance of the variables does a factor explain. An eigenvalue of more than one means that the factor explains more variance than a unique variable. An eigenvalue of 3 means that the factor would explain the variance of 3 variables, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Eigenvalue')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAspklEQVR4nO3deXhV5bn38e+dmQxM2WGeCYMiCIioEOfhWEVt36NVq1V7rFTr0EFPx/f01J6ec523PbWteKq11ap1qlbbWsdS6wSKCsgkKETGMIUECUkYk9zvH2uBIWbYQDYrO/v3ua59Ze21nr32nRDy22s9az2PuTsiIpK60qIuQEREoqUgEBFJcQoCEZEUpyAQEUlxCgIRkRSnIBARSXEKApEOwsyuMbNZUdchqUdBIJ2WmZWY2ZtmVmVmW81stpkdH3FNPzSzvWZWY2bbwvpOOoT9vGpmX05EjZJ6FATSKZlZV+BZYAbQE+gP3A7sPsj9ZLR/dfzB3fOBImAW8LSZWQLeRyQuCgLprEYCuPtj7l7v7jvd/W/uvmhfAzO7zsyWmVm1mS01s4nh+tVm9m0zWwTUmlmGmZ0YfnrfZmYLzey0RvvpZmb3mdlGM1tvZj82s/S2CnT3vcCDQB+gsOl2M5tiZu+GRzTvmtmUcP1/AicDd4VHFncdzg9KREEgndVyoN7MHjSzz5hZj8YbzewS4IfAVUBX4EKgslGTy4Hzge5Ab+A54McERxe3AU+ZWVHY9kGgDigGJgDnAG2etjGzbOAaoMzdK5ps6xm+550EIXEH8JyZFbr794E3gJvcPd/db4rj5yHSIgWBdEruvh0oARz4DbDFzJ4xs95hky8DP3H3dz1Q6u5rGu3iTndf5+47gSuB5939eXdvcPeZwFzgvHB/nwG+7u617l4O/By4rJXyPm9m24B1wHHAZ5tpcz6wwt1/7+517v4Y8AFwwSH9QERakYjznyIdgrsvI/jEjZmNBh4GfkHwaX8g8FErL1/XaHkwcImZNf4jnAm8Em7LBDY2Os2f1uT1TT3h7le2UX4/YE2TdWsI+jpE2pWCQFKCu39gZg8AXwlXrQOGt/aSRsvrgN+7+3VNG5lZX4IO6Ji717VTuQAbCEKmsUHAi83UJ3JYdGpIOiUzG21mt5rZgPD5QIIjgTlhk98Ct5nZcRYoNrOmf3j3eRi4wMz+yczSzSzHzE4zswHuvhH4G/AzM+tqZmlmNtzMTj3Mb+F5YKSZfSHsrL4UOJrgSiiAzcCww3wPEUBBIJ1XNXAC8LaZ1RIEwBLgVgB3fxL4T+DRsO2fCTqCP8Xd1wEXAd8DthAcIfwrn/z/uQrIApYCHwN/BPoeTvHuXglMC+utBL4FTGvUqfxL4GIz+9jM7jyc9xIxTUwjIpLadEQgIpLiFAQiIilOQSAikuIUBCIiKS7p7iOIxWI+ZMiQqMsQEUkq8+bNq3D3oua2JV0QDBkyhLlz50ZdhohIUjGzpneq76dTQyIiKU5BICKS4hQEIiIpTkEgIpLiFAQiIilOQSAikuIUBCIiKS5lguDDTdX8x7NL2bW3PupSREQ6lIQFQTh5xztmttDM3jez25tpc5qZVZnZgvDxg0TVs37bDu6btYp5az5O1FuIiCSlRN5ZvBs4w91rzCwTmGVmL7j7nCbt3nD3aQmsA4DJQwvJSDNmlVYwtTiW6LcTEUkaCQsCD2a8qQmfZoaPyGbByc/OYMKg7swurWi7sYhICkloH0E4v+sCoByY6e5vN9PspPD00QtmNqaF/Uw3s7lmNnfLli2HXE9JcRGL11exbceeQ96HiEhnk9AgcPd6dx8PDAAmm9kxTZrMBwa7+7HADIJ5Y5vbz73uPsndJxUVNTt4XlxKRhTiDm99VHnI+xAR6WyOyFVD7r4NeBU4t8n67e5eEy4/D2SaWcJO4I8b0J387Axm6fSQiMh+ibxqqMjMuofLXYCzgA+atOljZhYuTw7rSdjH9cz0NE4c1lP9BCIijSTyqqG+wINmlk7wB/4Jd3/WzK4HcPd7gIuBG8ysDtgJXBZ2MifM1OIYf19WzrqtOxjYMzeRbyUikhQSedXQImBCM+vvabR8F3BXompoTkl46ejs0goumzzoSL61iEiHlDJ3Fu9T3CufXgXZ6icQEQmlXBCYGSXFMd78qJKGhshuaxAR6TBSLggASkbE2Fq7h2WbtkddiohI5FIyCKY26icQEUl1KRkEvbvmMKJXPrNKdWOZiEhKBgEERwXvrKrUsNQikvJSNghKimPs2tvA/LUallpEUlvKBsEJw3qSnmbqJxCRlJeyQVCQk8n4gd3VTyAiKS9lgwCC00OLy7ZRtWNv1KWIiEQmtYNgRIwGh7dW6qhARFJXSgfB+IHdyctKVz+BiKS0lA6CzPQ0ThhWqHGHRCSlpXQQQHA/waqKWso+3hF1KSIikUj5INg3LPWbunpIRFJUygfByN75xPI1LLWIpK6UD4JgWOpCZpdWaFhqEUlJKR8EACUjiqis3cOHm6ujLkVE5IhTEABTiwsBDUstIqkpYUFgZjlm9o6ZLTSz983s9mbamJndaWalZrbIzCYmqp7W9O3WheFFeeonEJGUlMgjgt3AGe5+LDAeONfMTmzS5jPAiPAxHbg7gfW0qqQ4xtsrt7K7TsNSi0hqSVgQeKAmfJoZPpr2xl4EPBS2nQN0N7O+iaqpNVOLY+zcW897a7dF8fYiIpFJaB+BmaWb2QKgHJjp7m83adIfWNfoeVm4rul+ppvZXDObu2XLloTUeuLwQtJM/QQiknoSGgTuXu/u44EBwGQzO6ZJE2vuZc3s5153n+Tuk4qKihJQKXTNyeTYgd3VTyAiKeeIXDXk7tuAV4Fzm2wqAwY2ej4A2HAkampOSXGMheu2sX2XhqUWkdSRyKuGisyse7jcBTgL+KBJs2eAq8Krh04Eqtx9Y6JqaktJcTAs9ZyPNNyEiKSORB4R9AVeMbNFwLsEfQTPmtn1ZnZ92OZ5YCVQCvwG+GoC62nThEE96JKpYalFJLVkJGrH7r4ImNDM+nsaLTtwY6JqOFhZGWmcMKwnbygIRCSF6M7iJkqKY6zcUsuGbTujLkVE5IhQEDQxNRyWWqeHRCRVKAiaGNW7gFh+loJARFKGgqCJtDRjyvAYs0orCbowREQ6NwVBM0qKY1TU7Gb55pq2G4uIJDkFQTOmjgj6CXSXsYikAgVBM/p378KwWJ76CUQkJSgIWjC1OMaclZXsqWuIuhQRkYRSELRganGMHXvqWbBuW9SliIgklIKgBScNC4alVj+BiHR2CoIWdMvNZOyA7uonEJFOT0HQipLiQhas20a1hqUWkU5MQdCKqcUx6huct1dujboUEZGEURC04rjBPcjJTFM/gYh0agqCVmRnpDN5aKGCQEQ6NQVBG0qKCyktr2FT1a6oSxERSQgFQRs0LLWIdHYKgjYc1acrPfM0LLWIdF4KgjYEw1IH/QQallpEOqOEBYGZDTSzV8xsmZm9b2Zfa6bNaWZWZWYLwscPElXP4SgpjlFevZvScg1LLSKdT8ImrwfqgFvdfb6ZFQDzzGymuy9t0u4Nd5+WwDoOW0mjYalH9C6IuBoRkfaVsCMCd9/o7vPD5WpgGdA/Ue+XSAN65DKkMFf9BCLSKR2RPgIzGwJMAN5uZvNJZrbQzF4wszEtvH66mc01s7lbtmxJZKktCoal3sreeg1LLSKdS8KDwMzygaeAr7v79iab5wOD3f1YYAbw5+b24e73uvskd59UVFSU0HpbUlIco2Z3HQs1LLWIdDIJDQIzyyQIgUfc/emm2919u7vXhMvPA5lmFktkTYfqpOGFmIalFpFOKJFXDRlwH7DM3e9ooU2fsB1mNjmspzJRNR2O7rlZjO3fTf0EItLpJPKqoanAF4HFZrYgXPc9YBCAu98DXAzcYGZ1wE7gMu/AF+tPLY7xm9dXUrO7jvzsRP7oRESOnIT9NXP3WYC10eYu4K5E1dDeSopj3P3qR7yzqpIzRveOuhwRkXahO4sPwnGDe5CdkcasFR3y7JWIyCFREByEnMx0Jg/tyazSaC5hFRFJBAXBQZpaHGP55hrKt2tYahHpHBQEB6lk37DUH+nqIRHpHBQEB+novl3pnpupfgIR6TQUBAcpLc2YOjzGbA1LLSKdhILgEEwtjrFp+y4+2lIbdSkiIodNQXAISjR9pYh0IgqCQzCoMJdBPXM17pCIdAoKgkM0tTjGnI8qqdOw1CKS5OIKAjPrbWb3mdkL4fOjzezaxJbWsZUUx6jeXcfCsqqoSxEROSzxHhE8ALwE9AufLwe+noB6ksa+YanVTyAiyS7eIIi5+xNAA4C71wH1CasqCfTMy2JMv67qJxCRpBdvENSaWSHgAGZ2IpDy50SmFsd4b+3H1O6ui7oUEZFDFm8QfBN4BhhuZrOBh4CbE1ZVkigpjrG33nln9daoSxEROWRxzUfg7vPN7FRgFMEcAx+6+96EVpYEjh/Sk6yMNGavqOD0Ub2iLkdE5JDEFQRmdlWTVRPNDHd/KAE1JY2czHSOH9KDl5Zu4vrThhPLz466JBGRgxbvqaHjGz1OBn4IXJigmpLKl08eRvn23VwwYxYL1m2LuhwRkYMWVxC4+82NHtcBE4CsxJaWHE4f1YunbphCeprx+Xve4tG312owOhFJKod6Z/EOYERrDcxsoJm9YmbLzOx9M/taM23MzO40s1IzW2RmEw+xnkgd078bf72phBOHF/K9Py3m208tYtfelL66VkSSSLx9BH8lvHSUIDyOBp5o42V1wK1hR3MBMM/MZrr70kZtPkMQKCOAE4C7w69Jp0deFr+75nh+8fflzPhHKcs2VnP3lRMZ0CM36tJERFoVVxAA/9NouQ5Y4+5lrb3A3TcCG8PlajNbBvQHGgfBRcBDHpxLmWNm3c2sb/japJOeZtx6zijGDejON/+wgAtmzOLOyydw8oiiqEsTEWlRvH0ErzV6zG4rBJoysyEE/QpvN9nUH1jX6HlZuK7p66eb2Vwzm7tlS8efOP7so3vzzM0l9CrI4er73+F/XylVv4GIdFitBoGZVZvZ9mYe1Wa2PZ43MLN84Cng6+7e9DXWzEs+9RfT3e9190nuPqmoKDk+XQ+N5fGnG6dw/rh+/PSlD/nK7+exfVfK33ohIh1Qq0Hg7gXu3rWZR4G7d21r52aWSRACj7j70800KQMGNno+ANhwMN9AR5ablcGdl43nB9OO5uUPyvnsXbNZvrk66rJERA5wUFcNmVkvMxu079FGWwPuA5a5+x0tNHsGuCq8euhEoCpZ+wdaYmb8S8lQHv3yCWzfVcdn/3c2zy7qNFknIp1AvPMRXGhmK4BVwGvAauCFNl42FfgicIaZLQgf55nZ9WZ2fdjmeWAlUAr8BvjqIXwPSeGEYYU8d0sJo/sUcNOj7/HjZ5dqUhsR6RDivWroP4ATgb+7+wQzOx24vLUXuPssmu8DaNzGgRvjrCHp9e6aw+PTT+LHzy3lt7NWsXh9FXd9YSJFBRqaQkSiE++pob3uXgmkmVmau78CjE9cWZ1XVkYaP7roGO74/LEsWLeNC2bMYv7aj6MuS0RSWLxBsC28+ud14BEz+yXB/QRyiP7PxAE8/dUpZGYYl/76LX4/Z40uMRWRSMQbBBcRDCvxDeBF4CPggkQVlSrG9AuGpphaHOPf/ryE257U0BQicuTFGwTTgX7uXufuD7r7neGpIjlM3XOzuP/q47nlzBE8Nb+Mf777TdZt3RF1WSKSQuINgq7AS2b2hpndaGa9E1lUqklLM7559kjuu3oS67buYNqMWby2vOPfQS0inUO8Q0zc7u5jCK7w6Qe8ZmZ/T2hlKejMo3rz15tL6Nsth2t+9w4Pvrk66pJEJAUc7DDU5cAmoBLQ3IwJMLgwjz99dSpnju7ND//6Pq98UB51SSLSycV7Q9kNZvYq8DIQA65z93GJLCyVdclK587Lx3NUn67c8th7lJbXRF2SiHRi8R4RDCYYNG6Mu/97kzkFJAFyszL4zdWTyMpIY/pDc6naqQHrRCQx4u0j+A6w2Mz6xTvWkBy+/t27cPeVx7F26w5ueew96ht0n4GItL94Tw3dBGwGZgLPhY9nE1iXhCYP7cntF43hteVb+MmLH0Rdjoh0QvGONfR1YJTuHYjGFScMZtnG7fz69ZWM7lvA5yYMiLokEelE4u0jWAdUJbIQad2/XzCGE4b25NtPLWbhum1RlyMinUi8QbASeNXMvmtm39z3SGRhcqDM9DR+dcVEivKzmf77uZRv3xV1SSLSScQbBGsJ+geygIJGDzmCCvOz+c1Vk9i+s46vPDxP4xKJSLuIq4/A3W8HMLM8d69NbEnSmqP7deWOzx/LDY/M5//+eQk/vXgcwWRwIiKHJt6rhk4ys6XAsvD5sWb2q4RWJi36zNi+3HLmCP44r4z7Z6+OuhwRSXLxnhr6BfBPBENL4O4LgVMSVJPE4etnjuCco3vzn88t5Y0VGqBORA5d3GMNufu6JqtaPUFtZvebWbmZLWlh+2lmVtVoPuMfxFuLBCOW3nHpeIp75XPTo++xukJn7ETk0MR9+aiZTQHczLLM7DbC00SteAA4t402b7j7+PDxozhrkVB+dga/vep4zOC6h+ZSvUvDUIjIwYs3CK4nGIK6P1BGMF9xq5POu/vrwNbDKU7aNqgwl199YSIrK2r5xh8W0KBhKETkIMU71lCFu1/h7r3dvZe7X9lOdxmfZGYLzewFMxvTDvtLSVOKY/zb+Ufx92Xl3DFzedTliEiSievyUTO7s5nVVcBcd//LIb73fGCwu9eY2XnAn4ERLbz/dILpMhk0SGPdNefqKUNYtrGau14pZXTfAqaN6xd1SSKSJOI9NZRDcDpoRfgYB/QErjWzXxzKG7v7dnevCZefBzLNLNZC23vdfZK7TyoqKjqUt+v0zIwffXYMxw3uwW1PLmTJeo0IIiLxiTcIioEz3H2Gu88AzgKOAj4HnHMob2xmfSy8E8rMJoe1aFC7w5Cdkc7dV06kR24W0x+aS0XN7qhLEpEkEG8Q9AfyGj3PA/q5ez3Q7F8bM3sMeAsYZWZlZnatmV1vZteHTS4GlpjZQuBO4DJ3V0/nYepVkMO9X5xEZe0ebnh4HnvqGqIuSUQ6uHiHof4JsCCcrtIIbib7LzPLA5qdxN7dL29th+5+F3BX/KVKvMYO6MZPLh7H1x5fwA//+j7/9bmxUZckIh1YvGMN3WdmzwOTCYLge+6+Idz8r4kqTg7dReP7s2xjNfe89hFH9e3KF08cHHVJItJBtXpqyMxGh18nAn0J5iVYC/QJ10kH9q//NIozRvfi9mfeZ85Kdb+ISPOstdPyZvYbd7/OzF5pZrO7+xmJK615kyZN8rlz5x7pt01a23ft5XP/O5uPd+zlLzdOZWDP3KhLEpEImNk8d5/U7LZk659VEBy8lVtquOh/Z9O/exeeumEKednxdg2JSGfRWhC0dWroW42WL2my7b/apzxJtGFF+cy4fALLN1dz25MLNQyFiBygrctHL2u0/N0m29oaUE46kNNG9eI7nxnNC0s28a2nFrG7TrObiUigrXME1sJyc8+lg7vu5GHU7K7nzpdXsKaylruvPI5YfnbUZYlIxNo6IvAWlpt7Lh2cmfHNs0cy4/IJLCqr4qK7ZvPBpu1RlyUiEWsrCI41s+1mVg2MC5f3PdddSknqgmP78eT1J1HX0MA//+pNZi7dHHVJIhKhVoPA3dPdvau7F7h7Rri873nmkSpS2t+4Ad35y40lDO+Vz/Tfz+VXr5aSbFeQiUj7iHuqSul8+nTL4Q/TT+L8sX35yYsfcusTC9m1V53IIqlGF5SnuC5Z6cy4fAIjexdwx8zlrKqs5ddfPI5eBTlRlyYiR4iOCAQz45YzR3D3FRP5YGM1n71rtuYzEEkhCgLZ7zNj+/Lk9SfhwCX3vMWLSzZGXZKIHAEKAjnAMf278ZebpjKqTwHXPzyfGS+vUCeySCenIJBP6VWQw+PTT+Sz4/vxs5nLueXxBepEFunE1FkszcrJTOfnl45nZJ8CfvrSh6ytrOXeqybRu6s6kUU6Gx0RSIvMjK+eVsyvrzyOFeU1XHjXLBaVbYu6LBFpZwoCadM5Y/rw1A1TyEhL4/O/fotnF21o+0UikjQSFgRmdr+ZlZvZkha2m5ndaWalZrZIM551bEf17cpfbprKMf26cdOj73HHzOUazlqkk0jkEcEDtD5U9WeAEeFjOnB3AmuRdhDLz+aR607g4uMGcOfLK7jpsfns3KNOZJFkl7AgcPfXga2tNLkIeMgDc4DuZtY3UfVI+8jOSOenF4/j++cdxQtLNnHJr99kY9XOqMsSkcMQZR9Bf2Bdo+dl4bpPMbPpZjbXzOZu2bLliBQnLTMzrjtlGPdffTyrK3Zw4V2zmbdmK/U6VSSSlKK8fLS5iW2a/Uvi7vcC90IwZ3Eii5L4nT66F09/dQrXPvgu/3z3WwBkpafRJSudLpnp5GalkxN+7dJ4OTP9U226ZDXelrF/W/fcTPp260J6muZBEkmUKIOgDBjY6PkAQJejJJmRvQt45sYS/rxgPdt31rFzbz0794Rf9zbsX67ZXceW6t3s2lvPjj31Ybt66uI4isjKSGNwz1yGxPIYGstjSGEeQ2K5DI3l0bsghzSFhMhhiTIIngFuMrPHgROAKnfX4DZJqEdeFl+aOvSQXru3voEde+rZFQZD45DYubeeiprdrK6oZVVFLasra3lt+Rb21DXsf31OZhpDCsOAiOUxtDD4OiSWS1F+NmYKCZG2JCwIzOwx4DQgZmZlwL8DmQDufg/wPHAeUArsAL6UqFqk48pMT6NblzS6dYlvnqOGBmdD1U5WV+xgVWUtqyuCx4ebq5m5dPMBRxj52RkMLgyOHD45kgiWe+RmKiREQpZsA4pNmjTJ586dG3UZ0gHV1TewftvO4OihopbVlTtYFR5NlH28g8ZnoWL52Yzt35Vj+nfb/+jXLUfhIJ2Wmc1z90nNbdNYQ9JpZKSnMbgwj8GFeTDqwG176hpY9/GO/aeZlm2sZsn6Kl5bvmV/QPTMy2JMv66MDYNhbP9uDOjRReEgnZ6CQFJCVkYaw4vyGV6Uf8D6nXvqWbZpO++vr2Lx+iqWrN/Ova+v3H+KqWtOxv5QGBN+HdwzVx3U0qkoCCSldclKZ+KgHkwc1GP/ul1761m+uXp/MCxZX8XvZq9mT33QSV2QncHR/bruD4hj+ndlaCxfl7hK0lIQiDSRk5nOuAHdGTeg+/51e+oaWFEenE5asn47i9dX8fCcNewOr2DKzUrnqL5dGRbLY3BhbniKKpfBPfPolhtfR7hIVBQEInHIykhjTL9ujOnXjUuPD9bV1TdQuqVm/1HD0g3beXX5FrZU7z7gtd26ZDK4MJdBPXP3h8OgwmBZ90FIR6CrhkTa2Y49dazduoM1lTtYW7mDNVtrg+WtOyj7eOcBQ3FkZ6QxsGcuQwpzGdQzOIoYVJjL4J65DOiRS1aGRoqX9qGrhkSOoNysDEb36croPl0/tW1vfQMbtu1kTeUO1mzdwdrKT0JidmklOxtNCZpm0LdbF47q25Vvnj2So/t9en8i7UFBIHIEZTa+xLUJd2dLze7gKCIMijWVtcxaUcEFd83imilD+MbZI8nP1n9baV/6jRLpIMyMXgU59CrIYdKQnvvXV+3Yy09e+oD7Z6/i2UUb+LdpR3P+2L66v0HajU5AinRw3XIz+c/PjeXpG6YQy8/mpkff46r732F1RW3UpUknoSAQSRITBvXgmZtKuP3CMSxYu41zfvE6P5+5nF17NUucHB4FgUgSSU8zrp4yhJdvPZVzx/Thly+v4J9+8TqvLdeETXLoFAQiSahX1xzuvHwCj3z5BNLNuPr+d/jqI/PYVLUr6tIkCSkIRJLY1OIYL3z9ZG47ZyQvLyvnzJ+9ym/fWEldfUPbLxYJKQhEklx2Rjo3nTGCmd84lclDe/Lj55YxbcYs5q3ZGnVpkiQUBCKdxKDCXO6/5njuufI4qnbu5Z/vfotv/3ERH9fuibo06eAUBCKdiJlx7jF9+Ps3T+UrpwzjqfllnPGzV/nDu2tpiGN+aElNCgKRTigvO4PvnncUz91yMiN6FfDtpxZzya/fYtnG7VGXJh2QgkCkExvVp4A/fOVEfnrxOFZV1DJtxiz+49ml1Oyui7o06UASGgRmdq6ZfWhmpWb2nWa2n2ZmVWa2IHz8IJH1iKQiM+OSSQP5x62ncunxA7l/9irO/NmrPDB7Fe+u3krVjr1RlygRS9gw1GaWDiwHzgbKgHeBy919aaM2pwG3ufu0ePerYahFDs/8tR/zb39ewvsbPjlN1KsgmxG98xnRq4CRvQsY0Tufkb0KNKlOJxLVMNSTgVJ3XxkW8ThwEbC01VeJSEJNHNSDZ28uoezjnawor2bF5hqWb65hRXk1T8xdx449nwxZUVSQzcgwIEb0zmdk7wIFRCeUyCDoD6xr9LwMOKGZdieZ2UJgA8HRwftNG5jZdGA6wKBBgxJQqkhqMTMG9sxlYM9czhjde//6hgZn/badlJbXsHxzNcs311CqgOj0EhkEzY2R2/Q81HxgsLvXmNl5wJ+BEZ96kfu9wL0QnBpq5zpFJJSW9klAnD661/71DQ3Ohqqd4dFDNSvKa1ixufmAmDCwOyePiDG1OMbQWJ6Gy04CiQyCMmBgo+cDCD717+fu2xstP29mvzKzmLtXJLAuETlIaWnGgB7B9JktBcSK8mo+2FTNO6u28relmwHo1y2HkjAUphbHiOVnR/UtSCsSGQTvAiPMbCiwHrgM+ELjBmbWB9js7m5mkwmuYqpMYE0i0o6aCwh3Z+3WHcwqrWDWigpeen8zT8wtA2B0n4L9RwuTh/YkN0tzY3UECftXcPc6M7sJeAlIB+539/fN7Ppw+z3AxcANZlYH7AQu80RdxiQiR4SZ7Z+O84oTBlPf4Ly/oYo3VlQwu7SCB99cw2/eWEVmujFxUI/9wTC2fzcy0nVrUxQSdvlooujyUZHktnNPPe+u3srs0gpmlVbsv4y1ICeDk4YVqn8hQaK6fFRE5FO6ZKVzysgiThlZBEBlzW7e/KiS2aUVvLGi4oD+hanFMUpGxJgyPEZRgfoXEkVHBCLSYezrX9h3GunNjyqp2hnc+RzLz2JYLJ+hsTyGFuUxNJbHsFgegwpzyc5Ij7jyjk9HBCKSFBr3L1x54if9C3NWVlJaXsOqilpe/mAzFXM/GVo7zWBAj9wgIGJ5DAtDYmgsj37dupCWptNLbVEQiEiHlZ5mjBvQnXEDuh+wvmrnXlZX1LKqopaVFbWs3BKExLurtx5wX0N2Rtr+UGgcFMNi+fTIyzrC303HpSAQkaTTrUsmxw7szrEDux+w3t0pr97Nyi1BSKyqCALiw03VzFy6mbpGczJ0z83kqD5dueLEQZw7pk9KX7GkIBCRTsPM6N01h95dczhpeOEB2/bWN1D28U5WVdTsD4rZpRXc9Oh79O/ehWumDOHSyQPpmpN6w2Sos1hEUlZDg/PyB+XcN2slc1ZuJS8rnc8fP5AvTRnKoMLcqMtrV611FisIRESAJeuruH/WKp5ZuIEGd845ug/XnjyUSYN7dIr7GRQEIiJx2rx9Fw+9tZpH3l7Lth17OXZAN/6lZCjnje1LZhL3IygIREQO0s499Tw1v4z7Z61iZUUtfbvlcPWUIVx+/KCkHG5bQSAicogaGpxXl5dz36xVzC6tJDcrnUuOG8CXpg5lSCwv6vLipiAQEWkHSzds5/7Zq3hmwQb2NjRw1lG9ubZkKCcM7dnh+xEUBCIi7ai8ehcPv7WG389Zw8c79nJM/65cWzKU88f2IyujY/YjKAhERBJg1956/vTeeu6btYrS8hp6d83mqpOG8IXJgzrcncsKAhGRBGpocF5fsYX7Zq3ijRUVZKQZJSNinD+2L+eM6UO3LtF3LisIRESOkA83VfP0e2U8t2gjZR/vJDPdOGVEEeeP68tZR/eO7M5lBYGIyBHm7iwsq+K5RRt4btFGNlTtIis9jVNHFTFtXF/OPKo3+dlHbpQfBYGISIQaGpz31m3juUUbeX7xRjZt30VWRhqnjypi2rh+nDG6F3kJDgUFgYhIB9HQ4Mxf+zHPhqFQXr2bnMw0zhjdi2nj+nH6qF50yWr/iXYiCwIzOxf4JcHk9b919/9ust3C7ecBO4Br3H1+a/tUEIhIZ1Hf4MxdvZXnFm/k+cWbqKjZTZfMdM48qhfTxvXltFG9yMlsn1CIJAjMLB1YDpwNlAHvApe7+9JGbc4DbiYIghOAX7r7Ca3tV0EgIp1RfYPz9qpKnlu0kReXbKKydg95WemcdXRvzh/bl1NGFh1WKEQ1VeVkoNTdV4ZFPA5cBCxt1OYi4CEP0miOmXU3s77uvjGBdYmIdDjpacaU4TGmDI9x+4VjmLNyK88t3sCLSzbxlwUbKMjO4GtnjeDLJw9r9/dOZBD0B9Y1el5G8Km/rTb9gQOCwMymA9MBBg0a1O6Fioh0JBnpaZSMiFEyIsaPLjqGtz6q5NlFG+jTLScx75eQvQaaG3ij6XmoeNrg7vcC90JwaujwSxMRSQ6Z6WmcMrKIU0YWJew9EjkoRhkwsNHzAcCGQ2gjIiIJlMggeBcYYWZDzSwLuAx4pkmbZ4CrLHAiUKX+ARGRIythp4bcvc7MbgJeIrh89H53f9/Mrg+33wM8T3DFUCnB5aNfSlQ9IiLSvITeyubuzxP8sW+87p5Gyw7cmMgaRESkdR1z4GwRETliFAQiIilOQSAikuIUBCIiKS7pRh81sy3AmqjraCIGVERdxEFIpnqTqVZIrnqTqVZIrno7Yq2D3b3Zu9KSLgg6IjOb29JgTh1RMtWbTLVCctWbTLVCctWbTLWCTg2JiKQ8BYGISIpTELSPe6Mu4CAlU73JVCskV73JVCskV73JVKv6CEREUp2OCEREUpyCQEQkxSkIDoOZDTSzV8xsmZm9b2Zfi7qmtphZupm9Z2bPRl1LW8KpS/9oZh+EP+OToq6pJWb2jfB3YImZPWZmiZlK6hCZ2f1mVm5mSxqt62lmM81sRfi1R5Q17tNCrT8Nfw8WmdmfzKx7hCUeoLl6G227zczczGJR1BYvBcHhqQNudfejgBOBG83s6IhrasvXgGVRFxGnXwIvuvto4Fg6aN1m1h+4BZjk7scQDLt+WbRVfcoDwLlN1n0HeNndRwAvh887ggf4dK0zgWPcfRywHPjukS6qFQ/w6Xoxs4HA2cDaI13QwVIQHAZ33+ju88PlaoI/VP2jraplZjYAOB/4bdS1tMXMugKnAPcBuPsed98WaVGtywC6mFkGkEsHm2nP3V8HtjZZfRHwYLj8IPDZI1lTS5qr1d3/5u514dM5BLMZdggt/GwBfg58i2am3+1oFATtxMyGABOAtyMupTW/IPjFbIi4jngMA7YAvwtPZf3WzPKiLqo57r4e+B+CT34bCWba+1u0VcWl974ZAcOvvSKuJ17/ArwQdRGtMbMLgfXuvjDqWuKhIGgHZpYPPAV83d23R11Pc8xsGlDu7vOiriVOGcBE4G53nwDU0nFOXRwgPLd+ETAU6AfkmdmV0VbVOZnZ9wlOyT4SdS0tMbNc4PvAD6KuJV4KgsNkZpkEIfCIuz8ddT2tmApcaGargceBM8zs4WhLalUZUObu+46w/kgQDB3RWcAqd9/i7nuBp4EpEdcUj81m1hcg/FoecT2tMrOrgWnAFd6xb4AaTvChYGH4/20AMN/M+kRaVSsUBIfBzIzgHPYyd78j6npa4+7fdfcB7j6EoCPzH+7eYT+1uvsmYJ2ZjQpXnQksjbCk1qwFTjSz3PB34kw6aMd2E88AV4fLVwN/ibCWVpnZucC3gQvdfUfU9bTG3Re7ey93HxL+fysDJoa/0x2SguDwTAW+SPDpekH4OC/qojqRm4FHzGwRMB74r2jLaV541PJHYD6wmOD/VYcaYsDMHgPeAkaZWZmZXQv8N3C2ma0guLrlv6OscZ8War0LKABmhv/P7ml1J0dQC/UmFQ0xISKS4nREICKS4hQEIiIpTkEgIpLiFAQiIilOQSAikuIUBNIhmFkfM3vczD4ys6Vm9ryZjYy6rsNhZqeZWbM3lpnZNWbWYGbjGq1bEg5V0h7vXdMe+5HUoCCQyIU3Yf0JeNXdh7v70cD3gN7RVnbYTqP1O4zLCIYi6FDCgfMkhSgIpCM4Hdjr7vtvEnL3Be7+hgV+Gn5aXmxml8L+T9uvmdkTZrbczP7bzK4ws3fCdsPDdg+Y2T1m9kbYblq4PsfMfhe2fc/MTg/XX2NmT5vZi+E4/T/ZV5OZnWNmb5nZfDN7MhxjCjNbbWa3h+sXm9no8JP99cA3whugTm7m+34WGNPo7un9Gn+iN7OLzeyBRt/P3RbMg7HSzE61YDz8ZfvaNHrdz8KaXjazonDd8PB7mxf+TEY32u8dZvYK8P8O8t9PkpyCQDqCY4CWBsP7PwR3FR9LMKbPT/eNjxOu+xowluAO75HuPplgmO2bG+1jCHAqwRDc91gwacyNAO4+FrgceNA+mUxmPHBpuN9LLZiAKAb8X+Asd58IzAW+2eg9KsL1dwO3uftq4B7g5+4+3t3faOZ7awB+QnD0czB6AGcA3wD+SjDc8RhgrJmND9vkAfPDml4D/j1cfy9ws7sfB9wG/KrRfkeG39+tB1mPJDkdAkpHVwI85u71BIOkvQYcD2wH3t03jLKZfQTsG/p5McFRxj5PuHsDsMLMVgKjw/3OAHD3D8xsDcEfQggma6kK97sUGAx0B44GZgdnssgiGFZgn30DDs4jCK94PQp838yGHsRr/urubmaLgc3uvjis9X2C0FtAEDJ/CNs/DDwdHsFMAZ4MvweA7Eb7fTL8OUuKURBIR/A+cHEL26yF9QC7Gy03NHrewIG/203HUfGD2G99uC8DZrr75W28Zl/7uLh7nZn9jGBAtaY17tN02svG32fTn0FL7+0EZwC2ufv4FtrUtlmwdEo6NSQdwT+AbDO7bt8KMzvezE4FXic4PZMenuc+BXjnIPd/iZmlhf0Gw4APw/1eEb7XSGBQuL4lc4CpZlYcviY3jquaqgkGSmvLAwSnvYoardtsZkeZWRrwuTj20VQan4TrF4BZ4VwZq8zsEgg66c3s2EPYt3QyCgKJXDi2/OcIRsL8KDzF8UOC6R7/BCwCFhIExrcOYTjfDwnOk78AXO/uuwjOjaeHp1f+AFzj7rtb2oG7bwGuAR6zYDTUOQSnmFrzV+BzrXQW79v3HuBODpwh7DsEncn/IJj17GDVEnREzyPoT/hRuP4K4FozW0hwJHbRIexbOhmNPiqdWnglzbPu/seoaxHpqHREICKS4nREICKS4nREICKS4hQEIiIpTkEgIpLiFAQiIilOQSAikuL+P7+ApbHPluKTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "#Subset of the data, the columns containing our variables of interest\n",
    "x =df[df.columns[0:15]] \n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(x, 10)\n",
    "\n",
    "#Get eigenvalues and plot them\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev\n",
    "plt.plot(range(1,x.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Component Number')\n",
    "plt.ylabel('Eigenvalue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scree plot we generated, we can see that the biggest drop/elbow point seems to be at around 5 factors, so a good number of factors to choose might be 4 or so...\n",
    "\n",
    "Before running factor analysis, it is important to take note of an important hyperparameter unique to exploratory factor analysis, \"rotation,\" which is a bit of a difficult concept to explain but essentially the idea of rotation is to rotate the factors in order to achieve a simpler and more interpretable structure. There are a wide range of rotations which are possible to use in factor analysis: promax, varimax, etc.\n",
    "\n",
    "Promax is recommended to be used when you have suspicions that the factors are correlated -- varimax is recommended to be used when you don't have reason to believe that the factors are correlated in any way. Personally, I will be running factor analysis using promax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12625697 -0.06608265  0.91601482  0.16926259]\n",
      " [ 0.10495711 -0.06318687  0.6868701   0.10025261]\n",
      " [ 0.02216229  1.03544113 -0.08543871  0.05449775]\n",
      " [-0.09831732  0.54865844  0.07845617  0.04139875]\n",
      " [ 0.01105357 -0.02376685 -0.03408707 -0.13551256]\n",
      " [ 0.04099361 -0.01150913 -0.19088992  0.00974941]\n",
      " [-0.01638123  0.02175375  0.26755607 -0.18840557]\n",
      " [ 0.94747398  0.03859813  0.07070217  0.0083515 ]\n",
      " [ 0.35030889 -0.02990936  0.01243132 -0.00455254]\n",
      " [ 0.09099075  0.45667437 -0.0591554  -0.03102757]\n",
      " [ 0.67865145  0.014597    0.015227    0.0159134 ]\n",
      " [ 0.86487097  0.03984752 -0.01048584 -0.03482041]\n",
      " [ 1.01874491  0.04752093 -0.00400098 -0.03801498]\n",
      " [-0.03981746 -0.02975381 -0.04246248  0.98854297]\n",
      " [ 0.0322134  -0.04797332 -0.01959559  0.43101319]]\n"
     ]
    }
   ],
   "source": [
    "fa = FactorAnalyzer(4, rotation='promax')\n",
    "fa.fit(x)\n",
    "loads = fa.loadings_\n",
    "print(loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column represents a factor (we have 4 columns, as we specified 4 factors), and going down -- each of the 15 variables we used. The numbers are known as \"loadings,\" with higher loading indicating higher variable importance in regards to the factor. We can set an arbitrary cutoff point to aid us in picking important variables, for our analysis, this cutoff point will be abs(0.4).\n",
    "\n",
    "From the first factor (first column), we can see that variables 8, 11, 12, and 13 have loadings above abs(0.4).\n",
    "\n",
    "From the second factor, we can see that variables: 3, 4, and 10 have loadings above abs(0.4).\n",
    "\n",
    "From the third factor, we can see that variables 1 and 2 have loadings above abs(0.4).\n",
    "\n",
    "From the fourth factor, we can see that variables 14 and 15 have loadings above abs(0.4).\n",
    "\n",
    "Thus we can backtrack to find out which variables these correspond to:\n",
    "\n",
    "Factor 1: `Eng_country_yrs`, `nat_Eng`,\t`Lived_Eng_per`, and `Eng_little`\n",
    "\n",
    "Factor 2: `natlangs`, `primelangs`, and `currcountry`\n",
    "\n",
    "Factor 3: `age` and `age_group`\n",
    "\n",
    "Factor 4: `correct` and\t`skill_group`\n",
    "\n",
    "These variable abbreviations are a bit confusing to interpret, so for clarity, a more representative name will be chosen for ease of comprehension (alongside their loading value):\n",
    "\n",
    "__Factor 1__: \n",
    "\n",
    "Number of years living in English-speaking country (0.947)\n",
    "\n",
    "Is the speaker a *native* speaker of English? (0.679)\n",
    "\n",
    "Percentage of years speaking English that lived in English-speaking country (0.865)\n",
    "\n",
    "Language skills: values include monoeng (native speaker of English only), bileng (native speaker of English + at least one other lang), lot (immersion learner), little (non-immersion learner) (1.01)\n",
    "\n",
    "__Factor 2__: \n",
    "\n",
    "List of subject's native languages (1.04)\n",
    "\n",
    "List of subject's primary language now (0.549)\n",
    "\n",
    "Country currently lived in (0.457)\n",
    "\n",
    "__Factor 3__:\n",
    "\n",
    "Age (0.916)\n",
    "\n",
    "Age group (0.687)\n",
    "\n",
    "__Factor 4__:\n",
    "\n",
    "Proportion of correct answers on exam (0.989)\n",
    "\n",
    "Skill group (0.431)\n",
    "\n",
    "Possible interpretations for each factor could be of the following:\n",
    "\n",
    "__Factor 1__: English-related language skills\n",
    "\n",
    "__Factor 2__: General language proficiency\n",
    "\n",
    "__Factor 3__: Age information\n",
    "\n",
    "__Factor 4__: Testing results information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these factors are not that informative unfortunantly, such as factor 3 and factor 4 in which both variables in each of these factors are innately related to one another. After observing this, I had a worry about whether or not if some of the variables might be multicollinear with one another, so I wanted to implement a VIF (variance inflation factor) to see if some variables might benefit from being removed from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 1.778347\n",
       "age_group           1.644938\n",
       "natlangs            2.014643\n",
       "primelangs          1.602578\n",
       "psychiatric         1.020936\n",
       "education           1.039895\n",
       "Eng_start           1.221985\n",
       "Eng_country_yrs     5.411847\n",
       "house_Eng           1.145347\n",
       "currcountry         1.343599\n",
       "nat_Eng            14.585294\n",
       "Lived_Eng_per      56.127059\n",
       "Eng_little         88.355487\n",
       "correct             1.442113\n",
       "skill_group         1.273523\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cor = df.corr()\n",
    "vifs = pd.Series(np.linalg.inv(df.corr().values).diagonal(), index=df_cor.index)\n",
    "vifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIF score of an explanatory variable represents how well the variable is explained by the other explanatory videos. Strangely, from our VIFs, it isn't age/age_group and correct/skill_group which have high VIFs (which honestly is a bit weird, since they _should_ in theory have rather high VIFs? \n",
    "\n",
    "It actually shows that the variables `nat_Eng`, `Lived_Eng_per`, and `Eng_little`, have the highest VIF values of 14.58, 56.13, and 88.36 respectively. I will try removing the variable with the largest VIF, in this case, `Eng_little`, to see if that will help with multicollinearity in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                1.777490\n",
       "age_group          1.635998\n",
       "natlangs           2.010641\n",
       "primelangs         1.602502\n",
       "psychiatric        1.020931\n",
       "education          1.039142\n",
       "Eng_start          1.221585\n",
       "Eng_country_yrs    5.411706\n",
       "house_Eng          1.144474\n",
       "currcountry        1.335081\n",
       "nat_Eng            2.300520\n",
       "Lived_Eng_per      3.413774\n",
       "correct            1.437454\n",
       "skill_group        1.273398\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.drop(columns=[\"Eng_little\"])\n",
    "df2_cor = df2.corr()\n",
    "vifs2 = pd.Series(np.linalg.inv(df2.corr().values).diagonal(), index=df2_cor.index)\n",
    "vifs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping `Eng_little`, seems to have helped tremendously. The code is omitted for clarity, but I ran the previous classification algorithms above on this new dataset to see if any improvement occured but nothing significant seemed to change. \n",
    "\n",
    "This makes sense, since the removal of an explanatory variable which is already well-explained by the other variables doesn't really how the classification algorithm will run, but it is nice to remove just for clarity and reduction of dimensionality within our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "Most of the results are presented in-line with the code, so I won't go too in-depth in that regard. \n",
    "\n",
    "Significance:\n",
    "\n",
    "However, in regards to the significance of my work with factor analysis, there are some takeaways that we can obtain using the output from the factor analysis itself. The four factors we obtain in our final factor analysis run was: (1) English-related language skills, (2) General language proficiency, (3) Age information, and (4) Testing results information. This is a lot of easily consumable that staring at the 20+ variables which were present in the original dataset. Factor analysis allows us to condense down the variables by creating a latent variable and seeing which of variables in the dataset share some sort of commonality with one another. In regards to language learning, we can see that the most important aspects that are critical towards an individual's proficiency in langauge learning (in English) are easily summarized up in the output of factor analysis.\n",
    "\n",
    "Conclusion (and future work) :\n",
    "\n",
    "The first portion of my work which focuses on an attempt to create a classifer towards the language learning dataset in order to try to predict whether or not if an individual would score well on an English learning test using some demographic information turned out surprisingly well than I would have expected with a dataset of this calibur. Generally with large datasets, I tend to find creating models which perform well to be quite difficult as compared to smaller ones, so even with my models which averaged towards 60% accuracy was a big surprise to me, which I was happy with. \n",
    "\n",
    "With more time and work put into trying to perfect a classification model, I am definitely sure that future work can be conducted perhaps with using meta-learning strategies such hyperparameter tuning methods such as grid search, random search, or even SMBO (sequential model based optimization) tuning, would aid in the creation of a model which performs even better than the one I created. It would be interesting to see the tradeoff between the time that it takes to create models using these tuning methods and then comparing their performance metrics with each other. This could definitely warrant another side-study perhaps onto the feasibility of tuning methods with regards to dimensionality and/or classification which would be a deviation from this topic, but would be interesting nonetheless.\n",
    "\n",
    "The latter portion of my work which focuses on factor analysis has also been exhaustively discussed prior, however we can see that it is a very useful method which shares a lot of common traits with other dimensionality reduction methods such as principal component analysis (PCA) which is helpful in understanding a dataset better as these exploratory techniques allows one to understand a gain additional knowledge from running these methods. From our own language learning data case study, we were able to find out that most variance in the data could be explained with 4 factors which were all quite meaningful.\n",
    "\n",
    "I think a future study could most likely be conducted which focuses on trying to understand the different types of rotations that factor analysis deals with (varimax, promax, etc.) and seeing if there were potentially other types of rotations which might be more suited for this dataset. It would definitely be interesting to see if there are any improvements that can be made to better the factor analysis output which I came up with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related Works:\n",
    "    \n",
    "In Yong and Pearce's work, they discuss a brief overview as to how factor analysis works and also goes in a discussion into the mathematical aspects of how it functions alongside providing information as to how one should interpret factor loadings. In the article they discuss the arbitrary nature of cuttoff values and discuss using 0.32 as an acceptable cutoff value for large datasets as well. In their work, they conduct their own factor using STATA, factor analysis on a food risk and nutrition dataset. I wanted to try to somewhat validate their proposal as to cuttoff loadings being arbitrary, which was why I chose abs(0.4) as my own cutoff value in my report to try to assess whether or not their claim was valid. I ended up with my own interpretable factor analysis, so it does seem as if there isn't any specific cutoff values that one must choose in order to ensure a desirable output, it mostly falls to the user and their own judgement to decide what values seem plausible.\n",
    "\n",
    "A lot of the work that I performed was inspired by Pohlmann's work which reviewed and interpreted the usage of factor analysis is 25 articles involving educational research in which he found that most works dealt with EFA. He discusses a major problem in the articles regarding how the authors would oftentimes not provide a detailed enough explanation or overview of the variables they used in their factor analysis, which served as a warning during my own usage of the method. Many authors seemed to only focus on reporting the results of their analysis without providing much insight or interpretation of it. I sought to learn from these examples to ensure that my own factor analysis would be feasible to interpret for anyone reading the article who may not be as familiar with the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "\n",
    "Pohlmann, J. T. (2002). Use and Interpretation of Factor Analysis in The Journal of Education Research: 1992-2002. 1992–2002.\n",
    "\n",
    "Yong, A. G., & Pearce, S. (2013). A Beginner’s Guide to Factor Analysis: Focusing on Exploratory Factor Analysis. Tutorials in Quantitative Methods for Psychology, 9(2), 79–94. https://doi.org/10.1057/fsm.2014.17\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
